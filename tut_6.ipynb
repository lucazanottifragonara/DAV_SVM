{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tut_6.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Baprq14_EnUK",
        "wsKm_fZpFHBc",
        "tylEBwYGiskJ",
        "GWXqLd_5Jr2R",
        "o5j47nOhkM0M",
        "B1GdeBnuZ9MM",
        "HfnRt0dOooti",
        "2h4IhRmNo7Ko",
        "ic-5URLNU1ye",
        "VlfRE7iBRfb1"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucazanottifragonara/DAV_SVM/blob/main/tut_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Baprq14_EnUK"
      },
      "source": [
        "# NN Best Practices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsKm_fZpFHBc"
      },
      "source": [
        "## 1. Learning Rates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ax-PrtGFLdV"
      },
      "source": [
        "There is a subtle distinction to be made between types of hyperparameters. \n",
        "\n",
        "Optimization Hyperparameters:\n",
        "\n",
        "* **Learning Rate**\n",
        "* Batch Size\n",
        "* Epochs\n",
        "\n",
        "Architecture Hyperparameters:\n",
        "\n",
        "* Activation Functions\n",
        "* Number of neurons per layer\n",
        "* Number of layers\n",
        "\n",
        "Learning rate affects optimization convergence. Too big and the linear approximation of the loss used by gradient-based methods becomes useless. Too small and we get guaranteed, but slow convergence to a local optimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u752FXOMFwGv"
      },
      "source": [
        "We're going to look at some scenarios where the learning rate is too large/small so we can identify this phenomenon in practice and easily debug our networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRfbTVVcGdHb"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v75FkEaMhYvo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbv08xyRGgzG"
      },
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "        self.fc1 = torch.nn.Linear(100, 50)\n",
        "        self.fc2 = torch.nn.Linear(50, 20)\n",
        "        self.fc3 = torch.nn.Linear(20, 1)\n",
        "        \n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YqO2aA9HMiN"
      },
      "source": [
        "def create_dataset(n_samples, n_features):\n",
        "    x = torch.randn(n_samples, n_features)\n",
        "    y = torch.sum(x[:, :n_features // 2] / 10) ** 2 + torch.sum(x[:, :n_features // 2] / 10)\n",
        "    \n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_dh5kub1xuv"
      },
      "source": [
        "Run the next two cells for lr values of 100, 10, 1, 0.2, and 0.001. Notice that \n",
        "\n",
        "* 100, 10, and 1 both blow up the loss to infinity resulting in divergence that cannot be recovered from. \n",
        "* 0.2 results in \"convergence\" to a very bad loss value, likely due to sigmoid saturation\n",
        "* 0.001 Allows for significant loss minimization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HklsUn5bIoDP"
      },
      "source": [
        "torch.cuda.manual_seed(7)\n",
        "torch.manual_seed(7)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "x, y = create_dataset(1000, 100)\n",
        "\n",
        "model = Model()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.0) # Show lr of 100, 10, 1, and 0.1. Possible that lr of 1 converges to bad loss due to sigmoid saturation.\n",
        "criterion = torch.nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5Ezr-WPJMZa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3484
        },
        "outputId": "da7539cb-ad16-408e-a8b9-5b7e6de9383c"
      },
      "source": [
        "for i in range(200):\n",
        "    pred = model(x)\n",
        "    loss = criterion(pred, y)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    print(\"Loss: {}\".format(loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 616232.8125\n",
            "Loss: 600976.9375\n",
            "Loss: 567573.5\n",
            "Loss: 522713.90625\n",
            "Loss: 479982.0\n",
            "Loss: 440597.0\n",
            "Loss: 404398.0625\n",
            "Loss: 371157.0625\n",
            "Loss: 340641.96875\n",
            "Loss: 312632.75\n",
            "Loss: 286925.09375\n",
            "Loss: 263330.625\n",
            "Loss: 241675.9375\n",
            "Loss: 221801.75\n",
            "Loss: 203561.765625\n",
            "Loss: 186821.6875\n",
            "Loss: 171458.171875\n",
            "Loss: 157358.078125\n",
            "Loss: 144417.484375\n",
            "Loss: 132541.078125\n",
            "Loss: 121641.328125\n",
            "Loss: 111637.9375\n",
            "Loss: 102457.203125\n",
            "Loss: 94031.453125\n",
            "Loss: 86298.6015625\n",
            "Loss: 79201.6953125\n",
            "Loss: 72688.4140625\n",
            "Loss: 66710.7421875\n",
            "Loss: 61224.66796875\n",
            "Loss: 56189.75\n",
            "Loss: 51568.8671875\n",
            "Loss: 47328.015625\n",
            "Loss: 43435.90234375\n",
            "Loss: 39863.8828125\n",
            "Loss: 36585.6171875\n",
            "Loss: 33576.92578125\n",
            "Loss: 30815.6796875\n",
            "Loss: 28281.482421875\n",
            "Loss: 25955.71875\n",
            "Loss: 23821.208984375\n",
            "Loss: 21862.22265625\n",
            "Loss: 20064.345703125\n",
            "Loss: 18414.326171875\n",
            "Loss: 16899.990234375\n",
            "Loss: 15510.1884765625\n",
            "Loss: 14234.681640625\n",
            "Loss: 13064.0751953125\n",
            "Loss: 11989.71875\n",
            "Loss: 11003.7275390625\n",
            "Loss: 10098.8212890625\n",
            "Loss: 9268.3271484375\n",
            "Loss: 8506.126953125\n",
            "Loss: 7806.61474609375\n",
            "Loss: 7164.62646484375\n",
            "Loss: 6575.4296875\n",
            "Loss: 6034.69140625\n",
            "Loss: 5538.42333984375\n",
            "Loss: 5082.9580078125\n",
            "Loss: 4664.9541015625\n",
            "Loss: 4281.3271484375\n",
            "Loss: 3929.24755859375\n",
            "Loss: 3606.116943359375\n",
            "Loss: 3309.560302734375\n",
            "Loss: 3037.396484375\n",
            "Loss: 2787.60986328125\n",
            "Loss: 2558.36767578125\n",
            "Loss: 2347.977294921875\n",
            "Loss: 2154.885986328125\n",
            "Loss: 1977.6759033203125\n",
            "Loss: 1815.0340576171875\n",
            "Loss: 1665.7698974609375\n",
            "Loss: 1528.782470703125\n",
            "Loss: 1403.061279296875\n",
            "Loss: 1287.6761474609375\n",
            "Loss: 1181.7869873046875\n",
            "Loss: 1084.5997314453125\n",
            "Loss: 995.4076538085938\n",
            "Loss: 913.548828125\n",
            "Loss: 838.4243774414062\n",
            "Loss: 769.4744262695312\n",
            "Loss: 706.197021484375\n",
            "Loss: 648.11962890625\n",
            "Loss: 594.8222045898438\n",
            "Loss: 545.9063720703125\n",
            "Loss: 501.0115051269531\n",
            "Loss: 459.8122863769531\n",
            "Loss: 421.99688720703125\n",
            "Loss: 387.2945251464844\n",
            "Loss: 355.4449462890625\n",
            "Loss: 326.2135009765625\n",
            "Loss: 299.3856201171875\n",
            "Loss: 274.76519775390625\n",
            "Loss: 252.16978454589844\n",
            "Loss: 231.43296813964844\n",
            "Loss: 212.4007110595703\n",
            "Loss: 194.93544006347656\n",
            "Loss: 178.9029541015625\n",
            "Loss: 164.1912384033203\n",
            "Loss: 150.68783569335938\n",
            "Loss: 138.29696655273438\n",
            "Loss: 126.92333221435547\n",
            "Loss: 116.4861831665039\n",
            "Loss: 106.90792846679688\n",
            "Loss: 98.11527252197266\n",
            "Loss: 90.04596710205078\n",
            "Loss: 82.6405258178711\n",
            "Loss: 75.84407043457031\n",
            "Loss: 69.60684967041016\n",
            "Loss: 63.882869720458984\n",
            "Loss: 58.629478454589844\n",
            "Loss: 53.8081169128418\n",
            "Loss: 49.38275909423828\n",
            "Loss: 45.3214111328125\n",
            "Loss: 41.59512710571289\n",
            "Loss: 38.17409896850586\n",
            "Loss: 35.0352668762207\n",
            "Loss: 32.154117584228516\n",
            "Loss: 29.509540557861328\n",
            "Loss: 27.082700729370117\n",
            "Loss: 24.856046676635742\n",
            "Loss: 22.81139373779297\n",
            "Loss: 20.9356746673584\n",
            "Loss: 19.213781356811523\n",
            "Loss: 17.633535385131836\n",
            "Loss: 16.183988571166992\n",
            "Loss: 14.853011131286621\n",
            "Loss: 13.63149356842041\n",
            "Loss: 12.510251998901367\n",
            "Loss: 11.481982231140137\n",
            "Loss: 10.53785514831543\n",
            "Loss: 9.671058654785156\n",
            "Loss: 8.875880241394043\n",
            "Loss: 8.146117210388184\n",
            "Loss: 7.476184368133545\n",
            "Loss: 6.861515998840332\n",
            "Loss: 6.297511100769043\n",
            "Loss: 5.779265880584717\n",
            "Loss: 5.304327487945557\n",
            "Loss: 4.868122100830078\n",
            "Loss: 4.467776298522949\n",
            "Loss: 4.100468635559082\n",
            "Loss: 3.762998104095459\n",
            "Loss: 3.453662872314453\n",
            "Loss: 3.169698476791382\n",
            "Loss: 2.909209966659546\n",
            "Loss: 2.6700005531311035\n",
            "Loss: 2.4502570629119873\n",
            "Loss: 2.248730182647705\n",
            "Loss: 2.0638651847839355\n",
            "Loss: 1.8941928148269653\n",
            "Loss: 1.738452434539795\n",
            "Loss: 1.5954630374908447\n",
            "Loss: 1.4643558263778687\n",
            "Loss: 1.3439624309539795\n",
            "Loss: 1.2334061861038208\n",
            "Loss: 1.132066249847412\n",
            "Loss: 1.0389262437820435\n",
            "Loss: 0.9535921812057495\n",
            "Loss: 0.8751751184463501\n",
            "Loss: 0.8032309412956238\n",
            "Loss: 0.7372035980224609\n",
            "Loss: 0.6766151785850525\n",
            "Loss: 0.6210001111030579\n",
            "Loss: 0.5699372291564941\n",
            "Loss: 0.5230259299278259\n",
            "Loss: 0.48001018166542053\n",
            "Loss: 0.4406210482120514\n",
            "Loss: 0.404372900724411\n",
            "Loss: 0.371147096157074\n",
            "Loss: 0.3406635522842407\n",
            "Loss: 0.3126625716686249\n",
            "Loss: 0.286990761756897\n",
            "Loss: 0.2633545696735382\n",
            "Loss: 0.24169640243053436\n",
            "Loss: 0.22181400656700134\n",
            "Loss: 0.20359553396701813\n",
            "Loss: 0.18688517808914185\n",
            "Loss: 0.1715089976787567\n",
            "Loss: 0.15739528834819794\n",
            "Loss: 0.14445722103118896\n",
            "Loss: 0.13262109458446503\n",
            "Loss: 0.12171126157045364\n",
            "Loss: 0.11170945316553116\n",
            "Loss: 0.10255025327205658\n",
            "Loss: 0.09409917891025543\n",
            "Loss: 0.08635956794023514\n",
            "Loss: 0.07925590127706528\n",
            "Loss: 0.07277543842792511\n",
            "Loss: 0.06680943071842194\n",
            "Loss: 0.06135379523038864\n",
            "Loss: 0.05631246417760849\n",
            "Loss: 0.05169712007045746\n",
            "Loss: 0.04747027903795242\n",
            "Loss: 0.04357633739709854\n",
            "Loss: 0.040016040205955505\n",
            "Loss: 0.03671671450138092\n",
            "Loss: 0.03372320160269737\n",
            "Loss: 0.030964141711592674\n",
            "Loss: 0.02842523157596588\n",
            "Loss: 0.026113862171769142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tylEBwYGiskJ"
      },
      "source": [
        "### Rules of Thumb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yaWKq2-ixLc"
      },
      "source": [
        "1. Use Adam with initial learning rate of 0.001\n",
        "2. Use He normal initialization for relu neurons, and Glorot normal initialization for tanh neurons\n",
        "\n",
        "Here $n$ is the number of inputs to a layer and $m$ is the number of outputs, i.e. fan-in and fan-out\n",
        "\n",
        "He: $\\mathcal{N}(0, \\sqrt{\\frac{2}{n}})$\n",
        "\n",
        "Glorot: $\\mathcal{N}(0, \\sqrt{\\frac{2}{n + m}})$\n",
        "\n",
        "A bad weight initialization will lead to the same problems as a poorly set learning rate, though is much harder to debug since people don't put much thought into it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWXqLd_5Jr2R"
      },
      "source": [
        "## 2. Regularization "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAo90XxVR8AP"
      },
      "source": [
        "Recall that the point of regularization is to prevent overfitting. If we use too much though,  we end up with underfitting. These two concepts can be difficult to define precisely, but a possible indication of underfitting is lack of converengence to near perfect accuracy on the training set. \n",
        "\n",
        "Assuming your model has high enough capacity, it should be able to perfectly fit the training set. More regularization --> less capacity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ainV9HpxTW40"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoXcGUu5TrY6"
      },
      "source": [
        "def load_mnist(batch_size):\n",
        "    train_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "    \n",
        "    train_set = torchvision.datasets.MNIST(root=\"./\", train=True, download=True, transform=train_transform)\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    train_size = len(train_set)\n",
        "\n",
        "    test_set = torchvision.datasets.MNIST(root=\"./\", train=False, download=True, transform=train_transform)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "    test_size = len(test_set)\n",
        "    \n",
        "    data_loaders = {\"train\": train_loader, \"test\": test_loader}\n",
        "    dataset_sizes = {\"train\": train_size, \"test\": test_size}\n",
        "    \n",
        "    height, width = test_set.test_data.shape[1:]\n",
        "    channels = 1\n",
        "    classes = 10\n",
        "    \n",
        "    return data_loaders, dataset_sizes, height, width, channels, classes\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq564zVudhPn"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaCU4yafVAxf"
      },
      "source": [
        "class ClassificationModel(torch.nn.Module):\n",
        "    def __init__(self, height, width, channels, n_classes):\n",
        "        super(ClassificationModel, self).__init__()\n",
        "        \n",
        "        self.fc1 = torch.nn.Linear(height * width * channels, 200)\n",
        "        self.fc2 = torch.nn.Linear(200, 100)\n",
        "        self.fc3 = torch.nn.Linear(100, n_classes)\n",
        "        \n",
        "        self.relu = torch.nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sd48Q1LWuSa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "30c011bc-2d35-40bb-c821-1dadb505cf71"
      },
      "source": [
        "batch_size = 100\n",
        "data_loaders, dataset_sizes, height, width, channels, classes = load_mnist(batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQoqWDgx1cPX"
      },
      "source": [
        "Run the next two cells for the values of weight decay 10, 1 and 0.1 (stop after 3 or 4 epochs for each value since that will be enough to highlight the difference). Notice that\n",
        "\n",
        "* 10 and 1 lead to significant underfitting (pretty much random guessing) since too much emphasis put on making weights small. Thus little or no progress on either training or test sets\n",
        "* 0.1 after just a single epochs gets us to 75% train accuracy. This might still be too high if by epoch 100 lets say it hasn't cracked 90% accuracy, so we can't conclude anything about underfitting yet for this value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq0Ph-4iWoB7"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model = ClassificationModel(height, width, channels, classes)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.0, weight_decay=0.1) # Show weight decay of 10, 1 and 0.1. 10 underfits, 1 underfits, 0.1 is okay.\n",
        "epochs = 100\n",
        "use_gpu = True\n",
        "\n",
        "if use_gpu:\n",
        "    model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fXOHvEOXz0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1088
        },
        "outputId": "6937d39a-cf40-46d2-9919-09d57b83b1f2"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    for phase in [\"train\", \"test\"]:\n",
        "        if phase == \"train\":\n",
        "            model.train(True)\n",
        "        else:\n",
        "            model.train(False)\n",
        "            \n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        \n",
        "        for data in data_loaders[phase]:\n",
        "            x, y = data\n",
        "            \n",
        "            if use_gpu:\n",
        "                x, y = x.cuda(), y.cuda()\n",
        "                \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = model(x)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, y)\n",
        "            \n",
        "            if phase == \"train\":\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "            running_loss += loss.data.item() * x.size(0)\n",
        "            running_corrects += torch.sum(preds == y.data).item()\n",
        "            \n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_acc = running_corrects / dataset_sizes[phase]\n",
        "        \n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train Loss: 1.1814 Acc: 0.7672\n",
            "test Loss: 0.7693 Acc: 0.8596\n",
            "train Loss: 0.7197 Acc: 0.8616\n",
            "test Loss: 0.6643 Acc: 0.8738\n",
            "train Loss: 0.6662 Acc: 0.8713\n",
            "test Loss: 0.6403 Acc: 0.8760\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-4a4656ee5131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;31m# TODO: make efficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5j47nOhkM0M"
      },
      "source": [
        "### Rules of Thumb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFLcTDy-kQfm"
      },
      "source": [
        "1. Use no regularization to begin with. You need to make sure your model is able to fit the training data first, and then worry about generalization.\n",
        "2. Always remember to normalize your inputs: $\\frac{x - \\mu}{\\sigma}$\n",
        "\n",
        "What if we need regularization? How do we choose $\\lambda$ for L1 or L2 regularization?\n",
        "\n",
        "1. Look at the range of your loss function without regularization. I.e. observe if there is some upper bound $k$ s.t. $\\mathcal{L}_{class} < k$ after a few epochs\n",
        "2. Look at the magnitude of the regularization term $\\mathcal{L}_2$. Choose $\\lambda$ s.t. $\\lambda \\mathcal{L}_2 < k$\n",
        "\n",
        "Why? This makes sure that we don't care more about the regularization than we do about the actual task. Remember, this is just a starting point. If you want to squeeze out maximum performance, you need to do hyperparameter optimization  for $\\lambda$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1GdeBnuZ9MM"
      },
      "source": [
        "## Regularization Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmF8khPRiHZv"
      },
      "source": [
        "So we've talked about the basics like constraining weights which was taken from linear regression, and is not a NN specific regularization technique. \n",
        "\n",
        "How about something NN specific?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfnRt0dOooti"
      },
      "source": [
        "### BatchNorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hul8OaZjoqy1"
      },
      "source": [
        "Batch normalization is an implicit regularization technique in that it doesn't add an extra penalty to the loss function we are trying to optimize. Thus, there is no tradeoff to be balanced,  so no hyperparameter tuning required. But what is BatchNorm?\n",
        "\n",
        "BatchNorm was designed to address a phenomenon called \"internal covariate shift\" which means that even though we normalize the inputs to the network,\n",
        "the activations in the hidden layers can have their distributions change since the parameters of the network are being updated. Imagine you peform an update to the weights\n",
        "\n",
        "$$\\theta_{i+1} = \\theta_i - \\alpha \\nabla_{\\theta} \\mathcal{L}$$\n",
        "\n",
        "This gradient is for a particular mini-batch, and for a particular set of activations corresponding to that mini-batch. If you pass in the same mini-batch at the next step, the activations will have changed since the parameters changed. Thus, the gradient being computed changes, not only because we are in a different region of weight space, but also because\n",
        "\n",
        "Without any hidden layers, this is not a problem since the input distribution remains constant.\n",
        "\n",
        "![bn](https://cdn-images-1.medium.com/max/1600/1*Hiq-rLFGDpESpr8QNsJ1jg.png)\n",
        "\n",
        "Note that the last line says BN is not just making the activations have 0 mean and 1 variance. It actually for the learning of the best possible mean and variance with the parameters $\\gamma$ and $\\beta$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h4IhRmNo7Ko"
      },
      "source": [
        "### Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgTijQndN0ML"
      },
      "source": [
        "When applying dropout at train time to a given layer $i$ the activation $h^{(i)}_{j} = \\sigma(z^{(i)}_j)$ becomes $h^{(i)}_{j} = \\sigma(z^{(i)}_j) d^{(i)}_j$ for every neuron $h^{(i)}_j$ where $d^{(i)}_j \\sim Bernoulli(1- p)$ and $p$ is the dropout rate. This means any neuron has a chance of being omitted from the model for each mini-batch processed ($d$ is sampled from scratch for every mini-batch), so the connections going into it and out of it have no effect.\n",
        "\n",
        "At test time, we have to scale the activations since so that their expected value is the same as test time. I.e. we take $h^{(i)}_{j} = \\sigma(z^{(i)}_j) p$. Note that we are not dropping out neurons at test time.\n",
        "\n",
        "Dropout is a funny regularization technique with a couple of interpretations.\n",
        "\n",
        "1. If $n$ is the number of total neurons in the network, the number of neurons at any point during training is approx. $(1-p)n$ where $p \\in [0,1]$ is the dropout rate. Thus, the capacity is reduced in a stochastic way.\n",
        "2. Equally-weighted averaging of exponentially many models with shared weights. \n",
        "\n",
        "Regarding the 2nd interpretation, this \"ensembling\" method implies that neurons will be less correlated since the model cannot rely on the assumption that every neuron will always be present. So if a redundant linear combination of 2 neurons is what the typical SGD solution arrives at, dropout will remove that redundancy since their linear combination cannot be depended on.\n",
        "\n",
        "**CNNs**\n",
        "\n",
        "Dropout is not the right regularization technique for CNNs because units in conv layers are spatially correlated, so just dropping random ones doesn't really prevent information from flowing through."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic-5URLNU1ye"
      },
      "source": [
        "### DropBlock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abEC0gTPaowO"
      },
      "source": [
        "Very recent dropout technique for CNNs best explained with an image\n",
        "\n",
        "![bn](https://raw.githubusercontent.com/Jongchan/arxiv-screening/master/images/20181031-DropBlock-0.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebWEDvOfaugr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlfRE7iBRfb1"
      },
      "source": [
        "# Examples of Learning curves\n",
        "\n",
        "In this section, we look at training a few simple models and observing the effect of hyper-parameters. First, open Tensorflow Playground:\n",
        "\n",
        "[Tensorflow Playground](https://playground.tensorflow.org/)\n",
        "\n",
        "You should see 2 hidden-layer classification network. We will use this network to classify two concentric circles. We start by the default parameters, i.e. learning rate=0.03, no regularization, and Tanh activation units.\n",
        "\n",
        "1. Is this problem **linearly seperable**? **No**, why? Prove using convexity.\n",
        "* Click on Run and observe the training curves on the right.\n",
        "* Pause the training half-way and observe the form of the curve.\n",
        "  - Do train/test curves overlap? yes, so **no overfitting** yet.\n",
        "  - Has train/test curves flattened? not yet, so probably we can still improve.\n",
        "  - Look at the data and boundaries, have we learned the perfect boundary? if so, where is the additional improvement in train/test loss is going to come from? Increase in prediction confidence.\n",
        "* Continue the training until convergence.\n",
        "  - Where do you define convergence? The improvement in the loss is minimal.\n",
        "  - Is there a gap between train/test loss? No, why? because the data is generated without noise and comes from the same distribution\n",
        "* Rerun a few times and look at the train/test curves. Sometimes you should see a mid-way almost **flat regime** in the train/test loss. Can you guess why that happens? Hint: look at the heat-maps of the two neurons in the final layer. It is sometimes because the model has not learned to combine two neurons to get a solution, it has one good neuron and the other one has to learn to fill in the gap of the other one. In this process, that one good neuron might have to change a bit. Basically, we have reached a sub-optimal solution which is not a circle yet, and we have to combine the two neurons to get there and this search can take some wandering around in a **plateau**.\n",
        "  - More explanation: The flat regime starts early in the training and is brief with lr=0.03 but takes longer using lr=0.003. For the duration of this plateau, the boundary creates 3 disconnected regions, 2 red regions and one blue from two sides in one direction. The drop happens when the red regions are connected into 1 and the blue region is only unbounded by one side. https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.003&regularizationRate=0&noise=0&networkShape=4,2&seed=0.96502&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\n",
        "  \n",
        "  ![Plateau](https://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/tutorials/tut6/data/train_loss.png)\n",
        "* Now let's try to change a few settings.\n",
        "  - Use **ReLU** activations and retrain. How does the boundary compare to TanH boundaries? More linear boundaries.\n",
        "  - Use **Sigmoid** activations and retrain. Do you notice a difference in training speed compared ReLU and Tanh? In this example Sigmoid is almost 100x slower to train than ReLU. Generally, Sigmoid and TanH are more challenging to train when used in deep networks. We might need better initialization methods such as *Orthogonal initialization* to make them work. (Pennington et al. 2017) But they are commonly used in recurrent networks such as ones used in language modeling, where we have LSTMs. Note the challenge with Sigmoid units, multiplying the input by a constant can push the inputs to the saturation regime. So initialization is really important.\n",
        "  - But, don't yet give up on Sigmoid. Always make sure to **fine-tune hyper-parameters** after any change to your network. What if we increase the learning rate? Would Sigmoids converge faster? **Yes**, try learning rate 0.3.\n",
        "  - Let's test the model with some **noisy data**. Try increasing the noise bar on the left to 15. Train using 3 activations, ReLU, Sigmoid, Tanh and use learning 0.3. Notice how the training curves for ReLU and Tanh **fluctate** while for Sigmoid it doesn't. Now try learning rate 0.01. Do you still see any fluctation?\n",
        "* Let's try changing the architecture.\n",
        "  - Let's expand the model. Add 4 more hidden-layers with 2 units each to reach 6-hidden layers. Try training with ReLU, Sigmoid. Is it easier to train? **No**, even though the shallow models we trained are subsets of this deeper model (set new layers' weights to identity), it is more challenging to train the deeper models. This is the justification behind using **Residual connections in ResNets**.\n",
        "  - How about shrinking the model. Can you get high accuracy using only 1 hidden-layer and 2 ReLU activations? How about 3? Do the same for Sigmoid and Tanh. Is there a difference between the minimum number of units needed?\n",
        "* Let's try adding regularization.\n",
        "  - Try the spiral data. See how the model can **overfit**. Try adding regularization. For this data, use a 4-layer network, each with 6 ReLU units. To see overfitting, add noise and decrease the ratio of training examples to test examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFf5tkARl7xi"
      },
      "source": [
        "# Training curves from papers\n",
        "\n",
        "## Google neural machine translation (Yonghui 2016)\n",
        "GNMT is a translation system by Google used in http://translate.google.com. This model is an example where Sigmoid and Tanh activations are used. Here is the training curve for GNMT showing the loss vs steps.\n",
        "\n",
        "![GNMT loss vs steps](https://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/tutorials/tut6/data/gnmt_lp.png)\n",
        "\n",
        "A few interesting points in this plot:\n",
        "* Adam converges faster at the beginning with less fluctuations compared to SGD. In general, it is common to see Adam performing better at least at the beginning of the training compared to SGD for language modeling. Since GNMT, we now know that AdamW (Loshchilov 2017) can help fix the convergence of Adam in the end and also with its generalization.\n",
        "* To fix the gap between Adam and SGD, GNMT proposed to switch from Adam to SGD. This switch happens around $0.6\\times10^5$ steps. It is interesting to see that the loss temporarily goes up after the switch. In general, one has to be patient with training neural networks and do not judge based on a few iterations of training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9oK34uUmz_s"
      },
      "source": [
        "## Deep Residual Networks (He 2016)\n",
        "\n",
        "ResNets are one of the latest image classification models performing favorably on the ImageNet classification task. The network uses a hierarchy of Convolution layers, ReLUs, and a few other important keys ideas in helping with optimization i.e.\\ batch normalization, and residual connections, plus a special initialization for ReLU networks.\n",
        "\n",
        "![Resnet training curves](https://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/tutorials/tut6/data/resnet.png)\n",
        "\n",
        "Here thin curves are the training error and bold curves are test errors.\n",
        "Interesting points:\n",
        "* There are 3 learning rate reductions, each by a factor of $10$. After each learning rate reduction we see a drop in the error. It is important that using a large learning rate we progress fast but we would quickly plateau. We need to drop the learning rate to make less disturbing and subtle changes to the model.\n",
        "* For the larger model, Resnet-34, we see a gap between training and test performance at the end. Overfitting is only referred to when the validation loss goes up, not when it stays constant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av3FEYA0bfuu"
      },
      "source": [
        "# References\n",
        "\n",
        "1. Pennington, Jeffrey, Samuel Schoenholz, and Surya Ganguli. \"Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice.\" Advances in neural information processing systems. 2017.\n",
        "* Wu, Yonghui, et al. \"Google's neural machine translation system: Bridging the gap between human and machine translation.\" arXiv preprint arXiv:1609.08144 (2016).\n",
        "* Loshchilov, Ilya, and Frank Hutter. \"Fixing weight decay regularization in adam.\" arXiv preprint arXiv:1711.05101 (2017).\n",
        "* He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016."
      ]
    }
  ]
}